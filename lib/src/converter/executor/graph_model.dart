/**
 * @license
 * Copyright 2018 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */

// import {InferenceModel, io, ModelPredictConfig, NamedTensorMap, Tensor} from '@tensorflow/tfjs-core';

// import * as tensorflow from '../data/compiled_api';
// import {NamedTensorsMap, TensorInfo} from '../data/types';
// import {OperationMapper} from '../operations/operation_mapper';

// import {GraphExecutor} from './graph_executor';
// import {ResourceManager} from './resource_manager';

// ignore_for_file: unnecessary_this

import 'package:tensorflow_wasm/src/converter/executor/graph_executor.dart';
import 'package:tensorflow_wasm/src/converter/data/compiled_api.dart'
    as tensorflow;
import 'package:tensorflow_wasm/src/converter/executor/resource_manager.dart';
import 'package:tensorflow_wasm/src/io/types.dart';
import 'package:tensorflow_wasm/src/io/io.dart' as io;
import 'package:tensorflow_wasm/src/model_types.dart';
import 'package:tensorflow_wasm/src/tensor.dart' hide TensorInfo;

import '../operations/operation_mapper.dart';

class ModelHandler {
  final LoadHandler? load;
  final SaveHandler? save;
  final String? url;

  const ModelHandler.fromUrl(String this.url)
      : load = null,
        save = null;
  const ModelHandler.handlerLoad({
    required LoadHandler this.load,
    SaveHandler? this.save,
  }) : url = null;
  const ModelHandler.handlerSave({
    LoadHandler? this.load,
    required SaveHandler? this.save,
  }) : url = null;

  bool get isUrl => url != null;
}

const TFHUB_SEARCH_PARAM = '?tfjs-format=file';
const DEFAULT_MODEL_NAME = 'model.json';

/**
 * A `tf.GraphModel` is a directed, acyclic graph built from a
 * SavedModel GraphDef and allows inference execution.
 *
 * A `tf.GraphModel` can only be created by loading from a model converted from
 * a [TensorFlow SavedModel](https://www.tensorflow.org/guide/saved_model) using
 * the command line converter tool and loaded via `tf.loadGraphModel`.
 *
 * @doc {heading: 'Models', subheading: 'Classes'}
 */
class GraphModel implements InferenceModel {
  late final GraphExecutor _executor;
  String _version = 'n/a';
  late final io.IOHandler _handler;
  late final io.ModelArtifacts _artifacts;
  GraphExecutor? _initializer;
  final ResourceManager _resourceManager;
  late final tensorflow.ISignatureDef? _signature;

  final ModelHandler _modelUrl;
  final io.LoadOptions _loadOptions;

  // Returns the version information for the tensorflow model GraphDef.
  String get modelVersion {
    return this._version;
  }

  List<String> get inputNodes {
    return this._executor.inputNodes;
  }

  List<String> get outputNodes {
    return this._executor.outputNodes;
  }

  List<ModelTensorInfo> get inputs {
    return this._executor.inputs;
  }

  List<ModelTensorInfo> get outputs {
    return this._executor.outputs;
  }

  NamedTensorsMap get weights {
    return this._executor.weightMap;
  }

  Map<String, Map>? get metadata {
    return this._artifacts.userDefinedMetadata;
  }

  tensorflow.ISignatureDef? get modelSignature {
    return this._signature;
  }

  /**
   * @param modelUrl url for the model, or an `io.IOHandler`.
   * @param weightManifestUrl url for the weight file generated by
   * scripts/convert.py script.
   * @param requestOption options for Request, which allows to send credentials
   * and custom headers.
   * @param onProgress Optional, progress callback function, fired periodically
   * before the load is completed.
   */
  GraphModel(
    this._modelUrl, [
    this._loadOptions = const LoadOptions(),
  ]) : _resourceManager = ResourceManager();

  void _findIOHandler() {
    if (_modelUrl.load != null) {
      // Path is an IO Handler.
      this._handler = io.IOHandler(load: _modelUrl.load, save: _modelUrl.save);
    } else {
      final path = _modelUrl.url!;
      if (this._loadOptions.requestInit != null) {
        this._handler = io.httpHandler(path, this._loadOptions);
      } else {
        final handlers = io.getLoadHandlers([path], this._loadOptions);
        if (handlers.length == 0) {
          // For backward compatibility: if no load handler can be found,
          // assume it is a relative http path.
          handlers.add(io.httpHandler(path, this._loadOptions));
        } else if (handlers.length > 1) {
          throw Exception(
              "Found more than one (${handlers.length}) load handlers for " +
                  "URL '${[path]}'");
        }
        this._handler = handlers[0];
      }
    }
  }

  /**
   * Loads the model and weight files, construct the in memory weight map and
   * compile the inference graph.
   */
  Future<bool> load() async {
    this._findIOHandler();
    if (this._handler.load == null) {
      throw Exception(
          'Cannot proceed with model loading because the IOHandler provided ' +
              'does not have the `load` method implemented.');
    }
    final artifacts = await this._handler.load!();

    return this.loadSync(artifacts);
  }

  /**
   * Synchronously construct the in memory weight map and
   * compile the inference graph. Also initialize hashtable if any.
   *
   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}
   */
  bool loadSync(io.ModelArtifacts artifacts) {
    this._artifacts = artifacts;
    final graph = tensorflow.IGraphDef.fromJson(
        this._artifacts.modelTopology as Map<String, dynamic>);

    final Map<String, Object?>? signature =
        (this._artifacts.userDefinedMetadata?['signature'] ??
                this._artifacts.signature)
            ?.cast();
    this._signature =
        signature == null ? null : tensorflow.ISignatureDef.fromJson(signature);

    this._version =
        '${graph.versions!.producer}.${graph.versions!.minConsumer}';
    final weightMap = io.decodeWeights(
      this._artifacts.weightData!,
      this._artifacts.weightSpecs!,
    );
    this._executor = GraphExecutor(
        OperationMapper.Instance.transformGraph(graph, this._signature), null);
    this._executor.weightMap = this._convertTensorMapToTensorsMap(weightMap);
    // Attach a model-level resourceManager to each executor to share resources,
    // such as `HashTable`.
    this._executor.resourceManager = this._resourceManager;

    if (artifacts.modelInitializer != null &&
        artifacts.modelInitializer!['node'] != null) {
      final _g = tensorflow.IGraphDef.fromJson(
          this._artifacts.modelInitializer!.cast());
      final graph = OperationMapper.Instance.transformGraph(_g, null);
      final initializer = GraphExecutor(graph, null);
      this._initializer = initializer;
      initializer.weightMap = this._executor.weightMap;
      // Attach a model-level resourceManager to the initializer, the
      // hashTables created from when executing the initializer will be stored
      // in the resourceManager.
      initializer.resourceManager = this._resourceManager;
      initializer.executeAsync({}, []);
    }

    return true;
  }

  /**
   * Save the configuration and/or weights of the GraphModel.
   *
   * An `IOHandler` is an object that has a `save` method of the proper
   * signature defined. The `save` method manages the storing or
   * transmission of serialized data ("artifacts") that represent the
   * model's topology and weights onto or via a specific medium, such as
   * file downloads, local storage, IndexedDB in the web browser and HTTP
   * requests to a server. TensorFlow.js provides `IOHandler`
   * implementations for a number of frequently used saving mediums, such as
   * `tf.io.browserDownloads` and `tf.io.browserLocalStorage`. See `tf.io`
   * for more details.
   *
   * This method also allows you to refer to certain types of `IOHandler`s
   * as URL-like string shortcuts, such as 'localstorage://' and
   * 'indexeddb://'.
   *
   * Example 1: Save `model`'s topology and weights to browser [local
   * storage](https://developer.mozilla.org/en-US/docs/Web/API/Window/localStorage);
   * then load it back.
   *
   * ```js
   * const modelUrl =
   *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';
   * const model = await tf.loadGraphModel(modelUrl);
   * const zeros = tf.zeros([1, 224, 224, 3]);
   * model.predict(zeros).print();
   *
   * const saveResults = await model.save('localstorage://my-model-1');
   *
   * const loadedModel = await tf.loadGraphModel('localstorage://my-model-1');
   * console.log('Prediction from loaded model:');
   * model.predict(zeros).print();
   * ```
   *
   * @param handlerOrURL An instance of `IOHandler` or a URL-like,
   * scheme-based string shortcut for `IOHandler`.
   * @param config Options for saving the model.
   * @returns A `Promise` of `SaveResult`, which summarizes the result of
   * the saving, such as byte sizes of the saved artifacts for the model's
   *   topology and weight values.
   *
   * @doc {heading: 'Models', subheading: 'Classes', ignoreCI: true}
   */
  Future<io.SaveResult> save(
    ModelHandler handlerOrURL, {
    io.SaveConfig? config,
  }) async {
    if (handlerOrURL.isUrl) {
      final handlers = io.getSaveHandlers([handlerOrURL.url!]);
      if (handlers.length == 0) {
        throw Exception(
            "Cannot find any save handlers for URL '${handlerOrURL}'");
      } else if (handlers.length > 1) {
        throw Exception(
            "Found more than one (${handlers.length}) save handlers for " +
                "URL '${handlerOrURL}'");
      }
      handlerOrURL = ModelHandler.handlerSave(
        load: handlers[0].load,
        save: handlers[0].save,
      );
    }
    if (handlerOrURL.save == null) {
      throw Exception(
          'GraphModel.save() cannot proceed because the IOHandler ' +
              'provided does not have the `save` attribute defined.');
    }

    return handlerOrURL.save!(this._artifacts);
  }

  /**
   * Execute the inference for the input tensors.
   *
   * @param input The input tensors, when there is single input for the model,
   * inputs param should be a `tf.Tensor`. For models with mutliple inputs,
   * inputs params should be in either `tf.Tensor`[] if the input order is
   * fixed, or otherwise NamedTensorMap format.
   *
   * For model with multiple inputs, we recommend you use NamedTensorMap as the
   * input type, if you use `tf.Tensor`[], the order of the array needs to
   * follow the
   * order of inputNodes array. @see {@link GraphModel.inputNodes}
   *
   * You can also feed any intermediate nodes using the NamedTensorMap as the
   * input type. For example, given the graph
   *    InputNode => Intermediate => OutputNode,
   * you can execute the subgraph Intermediate => OutputNode by calling
   *    model.execute('IntermediateNode' : tf.tensor(...));
   *
   * This is useful for models that uses tf.dynamic_rnn, where the intermediate
   * state needs to be fed manually.
   *
   * For batch inference execution, the tensors for each input need to be
   * concatenated together. For example with mobilenet, the required input shape
   * is [1, 244, 244, 3], which represents the [batch, height, width, channel].
   * If we are provide a batched data of 100 images, the input tensor should be
   * in the shape of [100, 244, 244, 3].
   *
   * @param config Prediction configuration for specifying the batch size and
   * output node names. Currently the batch size option is ignored for graph
   * model.
   *
   * @returns Inference result tensors. The output would be single `tf.Tensor`
   * if model has single output node, otherwise Tensor[] or NamedTensorMap[]
   * will be returned for model with multiple outputs.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  Tensors predict(Tensors inputs, [ModelPredictConfig? config]) {
    return this.execute(inputs, this.outputNodes);
  }

  TensorMap _normalizeInputs(TensorsOrMap inputs) {
    if (inputs is TensorMap) {
      // The input is already a NamedTensorMap.
      return inputs;
    }
    final inputList = (inputs as Tensors).toTensorList();
    if (inputList.length != this.inputNodes.length) {
      throw Exception('Input tensor count mismatch,' +
          'the graph model has ${this.inputNodes.length} placeholders, ' +
          'while there are ${inputList.length} input tensors.');
    }
    int i = 0;
    return this.inputNodes.fold(TensorMap({}), (map, inputName) {
      map[inputName] = inputList[i++];
      return map;
    });
  }

  List<String> _normalizeOutputs(List<String>? outputs) {
    outputs = outputs ?? this.outputNodes;
    return outputs is String ? [outputs as String] : outputs;
  }

  /**
   * Executes inference for the model for given input tensors.
   * @param inputs tensor, tensor array or tensor map of the inputs for the
   * model, keyed by the input node names.
   * @param outputs output node name from the Tensorflow model, if no
   * outputs are specified, the default outputs of the model would be used.
   * You can inspect intermediate nodes of the model by adding them to the
   * outputs array.
   *
   * @returns A single tensor if provided with a single output or no outputs
   * are provided and there is only one default output, otherwise return a
   * tensor array. The order of the tensor array is the same as the outputs
   * if provided, otherwise the order of outputNodes attribute of the model.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  Tensors execute(
    TensorsOrMap inputs, [
    List<String>? outputs,
  ]) {
    final normalizedInputs = this._normalizeInputs(inputs);
    outputs = this._normalizeOutputs(outputs);
    final result = this._executor.execute(normalizedInputs, outputs);
    return result.length > 1 ? TensorList(result) : result[0];
  }

  /**
   * Executes inference for the model for given input tensors in async
   * fashion, use this method when your model contains control flow ops.
   * @param inputs tensor, tensor array or tensor map of the inputs for the
   * model, keyed by the input node names.
   * @param outputs output node name from the Tensorflow model, if no outputs
   * are specified, the default outputs of the model would be used. You can
   * inspect intermediate nodes of the model by adding them to the outputs
   * array.
   *
   * @returns A Promise of single tensor if provided with a single output or
   * no outputs are provided and there is only one default output, otherwise
   * return a tensor map.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  Future<Tensors> executeAsync(
    TensorsOrMap inputs, [
    List<String>? outputs,
  ]) async {
    final normalizedInputs = this._normalizeInputs(inputs);
    outputs = this._normalizeOutputs(outputs);
    final result = await this._executor.executeAsync(normalizedInputs, outputs);
    return result.length > 1 ? TensorList(result) : result[0];
  }

  /**
   * Get intermediate tensors for model debugging mode (flag
   * KEEP_INTERMEDIATE_TENSORS is true).
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  NamedTensorsMap? getIntermediateTensors() {
    return this._executor.getIntermediateTensors();
  }

  /**
   * Dispose intermediate tensors for model debugging mode (flag
   * KEEP_INTERMEDIATE_TENSORS is true).
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  void disposeIntermediateTensors() {
    this._executor.disposeIntermediateTensors();
  }

  NamedTensorsMap _convertTensorMapToTensorsMap(NamedTensorMap map) {
    return map.keys.fold({}, (newMap, key) {
      newMap[key] = [map[key]!];
      return newMap;
    });
  }

  /**
   * Releases the memory used by the weight tensors and resourceManager.
   *
   * @doc {heading: 'Models', subheading: 'Classes'}
   */
  void dispose() {
    this._executor.dispose();

    if (this._initializer != null) {
      this._initializer!.dispose();
    }

    this._resourceManager.dispose();
  }
}

/**
 * Load a graph model given a URL to the model definition.
 *
 * Example of loading MobileNetV2 from a URL and making a prediction with a
 * zeros input:
 *
 * ```js
 * const modelUrl =
 *    'https://storage.googleapis.com/tfjs-models/savedmodel/mobilenet_v2_1.0_224/model.json';
 * const model = await tf.loadGraphModel(modelUrl);
 * const zeros = tf.zeros([1, 224, 224, 3]);
 * model.predict(zeros).print();
 * ```
 *
 * Example of loading MobileNetV2 from a TF Hub URL and making a prediction with
 * a zeros input:
 *
 * ```js
 * const modelUrl =
 *    'https://tfhub.dev/google/imagenet/mobilenet_v2_140_224/classification/2';
 * const model = await tf.loadGraphModel(modelUrl, {fromTFHub: true});
 * const zeros = tf.zeros([1, 224, 224, 3]);
 * model.predict(zeros).print();
 * ```
 * @param modelUrl The url or an `io.IOHandler` that loads the model.
 * @param options Options for the HTTP request, which allows to send credentials
 *    and custom headers.
 *
 * @doc {heading: 'Models', subheading: 'Loading'}
 */
Future<GraphModel> loadGraphModel(
  ModelHandler modelUrl, [
  io.LoadOptions options = const io.LoadOptions(),
]) async {
  if (modelUrl == null) {
    throw Exception(
        'modelUrl in loadGraphModel() cannot be null. Please provide a url ' +
            'or an IOHandler that loads the model');
  }
  // if (options == null) {
  //   options = {};
  // }

  if (options.fromTFHub == true) {
    if ((modelUrl as io.IOHandler).load == null) {
      if (!(modelUrl.url!).endsWith('/')) {
        modelUrl = ModelHandler.fromUrl(modelUrl.url! + '/');
      }
      modelUrl = ModelHandler.fromUrl(
          '${modelUrl}${DEFAULT_MODEL_NAME}${TFHUB_SEARCH_PARAM}');
    }
  }
  final model = GraphModel(modelUrl, options);
  await model.load();
  return model;
}
